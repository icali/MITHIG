\newpage


The CMS experiment is mainly designed to record high luminosity, high-PU proton-proton collisions events.  However, with
several detectors tweaks/configuration changes, over the last several years, the CMS experiment has been able to record
successfully also the high multiplicity Heavy-Ions collisions delivered by LHC. As discussed already in previous
proposals, the main changes required to allow CMS to operate in HI environments could be summarized as follows:

\begin{itemize}
\item Dedicated Silicon Pixel Front End device firmware for high multiplicity environment. 
\item The Silicon Tracker zero suppression were bypassed and all the channels information were forwarded to the CMS DAQ system. The Zero Suppression was performed in HLT. A new tracker FED firmware was also designed to reduce the event payload of ~30% without information loss. 
\item DAQ reconfiguration / rebalance to deal with big data volumes and high throughput. In standard p-p collisions operation, an average event size correspond to ~ 1- 1.5 MB/even. During Pb-Pb collision operation, the average event size is at around 17 MB/event. 
\item The Level-1 trigger firmware was redesigned to perform a dedicated underlying event subtraction algorithm and to include specific HI algorithms 
\item Significant adjustment in the overall CMS dataflow
\end{itemize}


With the specific configuration described above and with a series of small adjustments not mentioned here, CMS was able
to run smoothly during the 2015 Pb-Pb data-taking with and average L1 rate of 10 kHz. Few hours of the Pb-Pb data-taking
period were also devoted to explore the limit of the system.  With the 2015  CMS configuration for Pb-Pb, the absolute
L1 rate limit is of 12 kHz. During the 2015 run, the collision rate was of ~20 kHz. For the 2018 Pb-Pb run and beyond
the rate limits could significantly impact the expected physics program. On one hand it would be beneficial to increase
the number of min bias events collected and on the other hand LHC will deliver a higher luminosity. We are considering a
collision rate of ~30 kHz already for 2018 Pb-Pb run. The CMS subsystems as well as their interaction with the DAQ have
been studied and a series of bottlenecks were identified. In this proposal the main interest area are specified
reporting also about the corresponding manpower needs. The preliminary goal is to increase the CMS readout rate to ~30
kHz. It is however important to bear in mind that this proposal will focus only on aspects that will potentially require
hardware changed.  The comprehensive series of tasks needed for the overall run preparation is not reported here. 




\section{Silicon Pixel Detector\label{sec:SiPixel}}
A new silicon pixel detector including 4 layers is being installed and commissioned. The detector FE electronics and
sensors is equivalent to the legacy detector while the whole off detector electronics was redesigned and based on the
latest FPGA technology (Xilinx virtex-7 family). The new firmware designs takes already into account high multiplicity
events. However, at present the performance of the latter has not been evaluated in HI multiplicity environment. An
electronics board with the scope to emulate the detector response was also designed to perform the commissioning of the
off detector electronics. The pixel group toke the responsibility to use this latter and define as soon as possible the
pixel operational parameters. A person in the HI group should however act a liaison person and provide the necessary
information required by the pixel group to perform the emulation. Accordingly, also the results should be evaluated and
possible solutions identified in case the performances are below expectations. As a rough estimation, a couple of months
should be enough for this activity.


\section{Silicon Strip Tracker\label{sec:SiTracker}}
In standard p-p collision operation, the tracker detector sends the full detector information to 439 Front End devices
(FEDs). They apply the common mode noise subtraction and the strip zero suppression. However the common noise
subtraction algorithm implemented in the tracker FEDs firmware doesn’t allow compensation of the baseline distortion
observed in high multiplicity HI events. The solution adopted up to now was to bypass the tracker zero suppression in
the tracker FEDs and implement a specific HI common mode noise subtraction and zero suppression algorithms as a HLT
process. However, this solution implies and heavy load on the link between the FEDs and the DAQ.  The whole
interconnection was indeed designed with a maximum of ~ 400 MB/s per FED (two links of 200 MB/s each for heavy load
FEDs).  Considering that in average fragment size it is at ~32 kB/event. The readout limit is at around 12 kHz. It was
also already demonstrated that the FED is designed to sustain up to 520 MB/s. In this optic the only solutions available
are either to increase the number of links between the tracker FEDs and the DAQ or to reduce the data payload. The first
solution appear to be too complicated adding other ~400 links. Apart the technical constraints, there would also not be
enough time to install and commission the links during the EYTS. The second option is to reduce the data payload. A FED
firmware version implemented already in 2015 allows the reduction from 10 to 8 the number of bits readout by each strip
ADC. However, this strategy would allow moderate increase of < 20 % of the overall L1 rate at the cost of a reduction of
the detector resolution/sensitivity. At present there are not conclusive studies on the effects of this change on the
overall tracking efficiency. The second strategy considered is to modify the FEDs firmware including a more refined
common mode subtraction algorithm accounting for baseline distortion.  At present this strategy is considered to be the
preferable considering the rate increase obtained. The system could run at > 50 kHz. The tracker FEDs are based on
Xilinx Virtex-2 FPGA that is quite limited in term of on chip resources as well as clock frequency. This limitation
implies that only algorithm designed with specific attention could be implemented in the FEDs FPGA. It also implies that
the FED firmware development should be branched between p-p and Pb-Pb operation not having enough on-chip resources to
maintain both features. In 2013 a specific HI zero suppression algorithm designed for high multiplicity environment and
that would satisfy the FPGA constraints was designed offline and it is named “baseline follower”.  Preliminary studies
performed by the tracker and HI groups showed promising performance results for the algorithm. The first part of the
project requires the detailed study of the algorithm offline. It will imply roughly 3 months of work and detailed
clustering and tracking studies. Hardware feasibility studies are going to be performed in parallel by tracker
engineers. During this phase the HI group should participate in providing the required information but the amount of
work could not be quantified yet. In the late spring/early summer 2017 the project will be handed over completely to the
HI group where a developer will implement the actual algorithm in firmware. We estimate that the activity of coding will
last between 6 and 8 months. Afterwards, three months commissioning time including tests on the bench and test at P5
should be considered. This schedule should leave some resources available for the actual 2018 HI run preparation. At
present the tracker group is focusing on the tracker operation during p-p collisions and the tracker upgrade project. In
this optic, a significant fraction of the work required for this project will need to fall under the HI group
responsibility as mentioned above. 


\section{ECAL\label{sec:ECAL}} 
The ECAL off detector electronics (DCC) was designed to cope with a L1 rate of 100 kHz and an average event size of 2
kB/event.  During p-p high-pu collisions the average event size is at around 1.5 – 1.8 kB/event. However, during Pb-Pb
collisions, the average event size is of 15-20 kB/event (depending by the trigger mix) and it is up to 34 kB/event for
central PbPb collisions. With so big event sizes, the L1 rate tolerated is up to 12 kHz. In the specific ECAL case, the
hardware limits can be identified in two main areas: the link between ECAL DCC and DAQ, and the internal DCC speed.  The
links to DAQ are standard S-Link designed for 200 MB/s. For the 2017 run an upgrade of the mezzanine card on the DCC
side including a bigger buffer is already planned. This upgrade should allow an increase of 10% in the total L1 rate.
Also the increase of clock frequency on the mezzanine card could be considered but the benefits are still to be
evaluated. The second limit encountered is the actual DCC internal speed firmware speed. An increase of 50 % (maybe also
100 %) in speed can be achieved modifying the internal DCC firmware structure allowing the firmware to run at a higher
clock frequency. The ECAL group engineers gave already their availability to participate and supervise the project.
However, the concrete implementation should be responsibility of the HI group. A person should be dedicated to the
project for roughly 6 months. Before envisaging the firmware change hypothesis, there are other two configurations that
should be evaluated. The ECAL group is already studying the calorimeter performance adjusting the Selective Readout and
Zero Suppression threshold. It is not clear yet how much the new settings could influence the event size for Pb-Pb
collisions but definitively a benefit could be directly obtained. On the same line, the ECAL event size could be reduced
up to 40 % reducing the number of samples collected for each event. At present the ECAL reads out 10 BX for each event
and this number can be programmed down to a minimum of 6 BX. However, reducing the number of samples could have strong
implication in the energy resolution. A complete study should be performed to understand how significant could be the
effect of x-fit in Pb-Pb collisions and the consequently effects on the physics program. Considering the amount of work
required for the offline studies mentioned above, we should allocate a person for one year to the project. Any of the
two offline solutions mentioned above indeed require specific performance studies and also the implementation of CMSSW
code for the eventually new configuration. Also the calorimeter calibrations should be re-derived. 

\section{L1 trigger\label{sec:L1Trigger}}
For the 2015 Pb-Pb run an upgraded L1 trigger system was designed. The Stage-1 L1 trigger was operated with specific HI
algorithms. However, in the middle of 2016 the full trigger upgrade (referred as Stage-2 in previous proposals) was
installed and commissioned. The upgraded trigger system has a new layer-1 and the single MP7 board layer-2 was replaced
with 9 MP7 boards operating with a time-multiplexed architecture. Also the firmware was completely redesigned by the L1
team including a series of new algorithm specific to p-p collision operation. In this optics, it is mandatory for the
2018 run to port to the new system the specific HI algorithm. In the list we have a specific background subtraction,
centrality triggers, single-track triggers and Q2 triggers. It was agreed with the L1 team that the stage-2 engineers
would take care of the actual firmware implementation and commissioning. However, it will be responsibility of the HI
group to provide support. It includes the performance studies of the algorithms and a specific description of the
implementation. Furthermore some of the several tests on the bench or at P5 will be performed by the HI group. Also the
compilation of the L1 and HLT menus will remain responsibility of the HI group. In term of manpower, it should be
considered a person for 6 months to follow the implementation process of the layer-2 and uGT firmware as well as
performing the performance studies needed for the algorithm implementation. The preparation of the L1 menu could be
considered as part of this time. 
=======
The electron/photon sort operation must determine the four highest transverse energy objects from 72 candidates supplied by the RCT, 
for both isolated and non-isolated electrons/photons.

To sort the jets the GCT must first perform jet finding and calibrate the clustered jet energies. The jets are created from the 396 
regional transverse energy sums supplied by the RCT. These are the sum of contributions from both the hadronic and electromagnetic 
calorimeters.

In addition to these tasks the GCT acts as a readout device for both itself and the RCT by storing information until receipt of a L1A 
and then sending the information to the DAQ.



